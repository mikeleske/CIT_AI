{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Naive Bayes for IMDB classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of your multinomial Naïve Bayes algorithm is to:\n",
    "\n",
    "1. **Build a model** by using the negative and positive movie reviews contained in the train folder.\n",
    "2. It should then take in the movie reviews in the folder test and **classify** them.\n",
    "3. You should **compare the predictions** of your model with the true class label and **calculate the accuracy of the model** for positive and negative movie reviews.\n",
    "\n",
    "The dataset contains 25,000 movie reviews in the train folder, split evenly between positive and negative. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from math import log\n",
    "\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 1 - File Parsing and Vocabulary Composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct words in allWords: 3604433, posWords: 2013363, negWords: 1925884\n",
      "Absolute number of words in allWords: 11605357, posWords: 5875296, negWords: 5730061\n"
     ]
    }
   ],
   "source": [
    "allWords = Counter()\n",
    "posWords = Counter()\n",
    "negWords = Counter()\n",
    "stats = defaultdict()\n",
    "\n",
    "classes = ['pos', 'neg']\n",
    "data = [('pos', posWords), ('neg', negWords)]\n",
    "\n",
    "\n",
    "def read_file(path, file):\n",
    "    return open(path+file, 'r', encoding='utf8').read().lower()\n",
    "\n",
    "def preprocess_reviews(txt):\n",
    "    REPLACE_NO_SPACE = re.compile(\"[.;:!\\'?,\\\"()\\[\\]]\")\n",
    "    REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "    \n",
    "    txt = REPLACE_NO_SPACE.sub(\"\", txt)\n",
    "    txt = REPLACE_WITH_SPACE.sub(\" \", txt)\n",
    "    \n",
    "    return txt\n",
    "\n",
    "def tokenize_input(txt, mode='re'):\n",
    "    words = []\n",
    "    txt = preprocess_reviews(txt)\n",
    "    \n",
    "    if mode == 're':\n",
    "        words = re.findall('\\w+', txt)\n",
    "    elif mode == 'str':\n",
    "        words = txt.split()\n",
    "    elif mode == 'nltk':\n",
    "        wordsFiltered = []\n",
    "        stopWords = set(stopwords.words('english'))\n",
    "        wordsToken = wordpunct_tokenize(txt)\n",
    "        \n",
    "        for w in wordsToken:\n",
    "            #if w not in stopWords:\n",
    "                wordsFiltered.append(w)\n",
    "        \n",
    "        stem = PorterStemmer()\n",
    "        #stem = WordNetLemmatizer()\n",
    "        for w in wordsFiltered:\n",
    "            rootWord = stem.stem(w)\n",
    "            words.append(rootWord)\n",
    "        \n",
    "        words = [' '.join(gram) for gram in ngrams(words,3)]\n",
    "        #words += re.findall('\\w+', txt)\n",
    "        words += txt.split()\n",
    "        \n",
    "        #print(txt)\n",
    "        #print(words)\n",
    "        \n",
    "    return words\n",
    "\n",
    "def count(c, cnt, stats):\n",
    "    path = './data/train/{}/'.format(c)\n",
    "    files = os.listdir(path)\n",
    "    stats[c] = len(files)\n",
    "    \n",
    "    for file in files:\n",
    "        words = tokenize_input(read_file(path, file), mode='nltk')\n",
    "        allWords.update(words)\n",
    "        cnt.update(words)\n",
    "\n",
    "for c, cnt in data:\n",
    "    count(c, cnt, stats)\n",
    "        \n",
    "print('Number of distinct words in allWords: {}, posWords: {}, negWords: {}'.format(\n",
    "    len(allWords), len(posWords), len(negWords)))\n",
    "print('Absolute number of words in allWords: {}, posWords: {}, negWords: {}'.format(\n",
    "    len(list(allWords.elements())), len(list(posWords.elements())), len(list(negWords.elements()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct words in allWords: 3604433, posWords: 3604433, negWords: 3604433\n",
      "Absolute number of words in allWords: 11605357, posWords: 5875296, negWords: 5730061\n"
     ]
    }
   ],
   "source": [
    "# Ensure that all words seen during training are included in both dictionaries (posWords and negWords)\n",
    "for w in list(allWords):\n",
    "    if not posWords[w]: posWords[w] = 0\n",
    "    if not negWords[w]: negWords[w] = 0\n",
    "\n",
    "print('Number of distinct words in allWords: {}, posWords: {}, negWords: {}'.format(\n",
    "    len(allWords), len(posWords), len(negWords)))\n",
    "print('Absolute number of words in allWords: {}, posWords: {}, negWords: {}'.format(\n",
    "    len(list(allWords.elements())), len(list(posWords.elements())), len(list(negWords.elements()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 2 – Word Probability Calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"multinomial_bayes.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(data, posWords, negWords, allWords):\n",
    "    # Add total number of positive words, negative words and vocabulary to stats\n",
    "    stats['w_pos'] = len(list(posWords.elements()))\n",
    "    stats['w_neg'] = len(list(negWords.elements()))\n",
    "    stats['voc'] = len(allWords)\n",
    "\n",
    "    # Create a fancy probability dictionary p_\n",
    "    # Insert prior class probability into p_\n",
    "    stats['p_'] = defaultdict()\n",
    "    stats['p_']['p_pos'] = stats['pos'] / (stats['pos'] + stats['neg'])\n",
    "    stats['p_']['p_neg'] = stats['neg'] / (stats['pos'] + stats['neg'])\n",
    "    \n",
    "    for c, cnt in data:\n",
    "        print('Calculating conditional probalibities: ', c)\n",
    "        for k, v in cnt.items():\n",
    "            p_id = 'p_{}_{}'.format(c, k)\n",
    "            stats['p_'][p_id] = (v + 1)/(stats['w_'+c] + stats['voc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating conditional probalibities:  pos\n",
      "Calculating conditional probalibities:  neg\n"
     ]
    }
   ],
   "source": [
    "fit(data, posWords, negWords, allWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 3 – Classifying Unseen Documents and Basic Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(c, words):\n",
    "    prediction = defaultdict()\n",
    "    prediction['pos'] = log(stats['p_']['p_pos'])\n",
    "    prediction['neg'] = log(stats['p_']['p_neg'])\n",
    "    \n",
    "    for cl in classes:\n",
    "        for w in words:\n",
    "            p_id = 'p_{}_{}'.format(cl, w)\n",
    "            if p_id in stats['p_']:\n",
    "                prediction[cl] += log(stats['p_'][p_id])\n",
    "\n",
    "    return c == max(prediction, key=prediction.get)\n",
    "\n",
    "\n",
    "def test(c, cnt):\n",
    "    path = './data/test/{}/'.format(c)\n",
    "    files = os.listdir(path)\n",
    "    correct = 0\n",
    "    \n",
    "    for file in files:\n",
    "        words = tokenize_input(read_file(path, file), mode='nltk')\n",
    "        if predict(c, words):\n",
    "            correct +=1\n",
    "\n",
    "    print('Accuracy for class {}: {}'.format(c, correct/len(files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for class pos: 0.8475\n",
      "Accuracy for class neg: 0.9025\n"
     ]
    }
   ],
   "source": [
    "for c, cnt in data:\n",
    "    test(c, cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Some result history**\n",
    "\n",
    "```\n",
    "NLTK 3gram + STR + NOSTOP + STEM + preprocess\n",
    "Accuracy for class pos: 0.8475\n",
    "Accuracy for class neg: 0.9025\n",
    "\n",
    "NLTK 3gram + STR + NOSTOP + STEM\n",
    "Accuracy for class pos: 0.838\n",
    "Accuracy for class neg: 0.907\n",
    "\n",
    "NLTK\n",
    "Accuracy for class pos: 0.793\n",
    "Accuracy for class neg: 0.9085\n",
    "\n",
    "NLTK + RE\n",
    "Accuracy for class pos: 0.8095\n",
    "Accuracy for class neg: 0.8965\n",
    " \n",
    "NLTK + STR\n",
    "Accuracy for class pos: 0.8095\n",
    "Accuracy for class neg: 0.8965\n",
    "    \n",
    "NLTK + STR + RE\n",
    "Accuracy for class pos: 0.801\n",
    "Accuracy for class neg: 0.891\n",
    "\n",
    "NLTK 3gram + STR + STOP + STEM + preprocess\n",
    "Accuracy for class pos: 0.809\n",
    "Accuracy for class neg: 0.8645\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
